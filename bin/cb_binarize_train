#!/usr/bin/env python3

import PIL
import fastai.vision
import tormentor
import glob
from tormentor import *
import matplotlib
matplotlib.use('TkAgg')
from matplotlib import pyplot as plt
import torch
import fargv
import torchvision
import numpy as np
from cbbin import *

from PIL import Image
import time
import tqdm
import sys
from kornia.losses import dice_loss


torch.multiprocessing.set_start_method('spawn', force=True)

torch.autograd.set_detect_anomaly(True)

p={
    "n_channels": 3,
    "n_classes": 2,
    "gt_focus_loss": 0.,
    "save_images": False,
    "save_input_images": False,
    "self_pattern": "*png",
    "arch": [("dunet34","dunet18","dunet50","unet", "R2AttUNet", "AttUNet", "R2UNet", "UNet","runet"),"Model Archtecture"],
    "rrds_root": "/home/anguelos/data/rr/focused_segmentation/zips",
    "dataset": [("rrds", "self2017", "dibco2010", "dibco2011", "dibco2012", "dibco2013", "dibco2014", "dibco2016", "dibco2017", "dibco2018", "dibco2019"), "Either Robust Reading Segmentation (rrds), or Document Image Binarization"],
    "val_augmentation": "",
    "train_augmentation": "(RandomPlasmaLinearColor & RandomWrap.custom(roughness=Uniform(value_range=(0.1, 0.7)), intensity=Uniform(value_range=(0.18, 0.62))) & RandomPlasmaShadow.custom(roughness=Uniform(value_range=(0.334, 0.72)), shade_intencity=Uniform(value_range=(-0.32, 0.0)), shade_quantity=Uniform(value_range=(0.0, 0.44))) & RandomPerspective.custom(x_offset=Uniform(value_range=(0.75, 1.0)), y_offset=Uniform(value_range=(0.75, 1.0))) & RandomPlasmaBrightness.custom(roughness=Uniform(value_range=(0.1, 0.4)),intencity=Uniform(value_range=(0.322, 0.9))) )",
    "optimiser":[("adam", "sgd"),"The optimiser used for training"],
    "loss": [("dice", "bce"), "The optimiser used for training"],
    "io_threads": 1,
    "log_freq": 10,
    "lr": .001,
    "epochs": 10,
    "tormentor_device": "cpu",
    "device": "cuda",
    "val_device": "{device}",
    "validate_freq": 5,
    "trainoutputs_freq": 5,
    "archive_nets": False,
    "batch_size": 1,
    "save_freq": 10,
    "mask_gt": 1,
    "resume_fname": "./models/{arch}.pt",
    "patch_width": 512,
    "patch_height": 512,
    "val_patch_width": -1,
    "val_patch_height": -1,
    "dice_coefficinet":.0,
    #"rnd_pad":False,
    "crop_loss": 0,
    "pretrained": True,
    #"bn_momentum": (.1, "[0.-1.] negative for None this changes the batchnormalisation momentum parameter.")
}
param_dict, _ = fargv.fargv(p.copy(), argv=sys.argv.copy(), return_named_tuple=False)
p, _ = fargv.fargv(p, return_named_tuple=True)
device = torch.device(p.device)


def run_epoch(p, device, loader, net, criterion, optimizer=None, save_images=True, is_deeplabv3=True, save_input_images=False):
    is_validation = optimizer is None
    isval_str = 'validation' if is_validation else 'training'
    with torch.set_grad_enabled(not is_validation):
        net = net.to(device)
        if is_validation:
            net.eval()
        else:
            net.train()
        fscores = []
        precisions = []
        recalls = []
        losses = []

        t=time.time()

        for n, (input_img,gt,mask) in tqdm.tqdm(enumerate(loader)):
            #plt.imshow(gt.cpu().numpy()[0,1:,:,:]);plt.colorbar();plt.show()
            input_img, gt, mask = input_img.to(device), gt.to(device), mask.to(device)

            coeffs = mask.clone()
            if p.gt_focus_loss > 0:
                coeffs *= (1-p.gt_focus_loss) + p.gt_focus_loss * gt[:, 1:, :, :]
            if p.crop_loss>0:
                coeffs[:, :, :p.crop_loss,:]=0
                coeffs[:, :, -p.crop_loss:,:]=0
                coeffs[:, :, :,:p.crop_loss]=0
                coeffs[:, :, :,-p.crop_loss:]=0

            prediction = net(input_img)
            loss = criterion(prediction, gt)
            loss = loss * coeffs
            if p.dice_coefficinet > 0:
                soft_pred = torch.softmax(prediction, dim =1)
                dice = 1-((2*soft_pred*coeffs*gt*coeffs).sum()+.1)/((soft_pred*coeffs+gt*coeffs).sum()+.1)
                loss = p.dice_coefficinet * dice + (1-p.dice_coefficinet) * loss

            prediction = torch.nn.functional.softmax(prediction, dim=1)
            loss = loss.sum()
            if not is_validation:
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
            confusion, precision, recall, fscore = render_confusion(prediction[0,0,:,:]<prediction[0,1,:,:], gt[0, 0, :, :] < .5, mask[0,0,:,:] > .5)

            fscores.append(fscore)
            precisions.append(precision)
            recalls.append(recall)
            losses.append(loss.item()/gt.view(-1).size()[0])
            if save_images:
                print("Conf:",confusion.shape, confusion.min(),confusion.max())
                conf_img=Image.fromarray((confusion[:,:,[2,1,0]]).astype("uint8")).convert('RGB')
                conf_img.save(f"/tmp/conf_{isval_str}_{n}.png")
                prediction = torch.softmax(prediction[0, :, :, :], dim=0)
                Image.fromarray((prediction[0,:,:].detach().cpu().numpy() * 255)).convert('RGB').save(f"/tmp/output_{isval_str}_{n}.png")
                Image.fromarray((mask[0, 0, :, :].detach().cpu().numpy() * 255)).convert('RGB').save(f"/tmp/mask_{isval_str}_{n}.png")
                Image.fromarray((gt[0, 1, :, :].detach().cpu().numpy() * 255)).convert('RGB').save(f"/tmp/gt_{isval_str}_{n}.png")
                print(coeffs.shape,coeffs.min(),coeffs.max())
                Image.fromarray((coeffs[0, 0, :, :].detach().cpu().numpy() * 255).astype("uint8")).convert('RGB').save(
                    f"/tmp/coeff_{isval_str}_{n}.png")
            if save_input_images:
                torchvision.transforms.ToPILImage()(input_img[0,:,:,:].detach().cpu()).save(f"/tmp/input_{isval_str}_{n}.png")

    lines = []
    lines.append("Epoch {} {} Total:\t{:05f}%".format(epoch, isval_str, 100*sum(fscores)/(.0000001+len(fscores))))
    lines.append('')
    print("N:\t{} % computed in {:05f} sec.".format(isval_str, time.time()-t))
    print("\n".join(lines))
    return sum(fscores) / len(fscores),sum(precisions) / len(precisions), sum(recalls) / len(recalls),sum(losses) / len(losses)


if p.train_augmentation != "":
    train_augmentation_factory = eval(p.train_augmentation)
else:
    train_augmentation_factory = tormentor.RandomIdentity

if p.val_augmentation != "":
    val_augmentation_factory = eval(p.val_augmentation)
else:
    val_augmentation_factory = tormentor.RandomIdentity

if p.patch_width != -1 or p.patch_height != -1:
    train_augmentation_factory = train_augmentation_factory.new_size(p.patch_width, p.patch_height)

if p.val_patch_width != -1 or p.val_patch_height != -1:
    val_augmentation_factory = val_augmentation_factory.new_size(p.val_patch_width, p.val_patch_height)

if p.dataset == "rrds":
    trainset = RR2013Ch2(train=True, return_mask=True,cache_ds=True,root=p.rrds_root,default_width=p.patch_width,default_height=p.patch_height)
    validationset = RR2013Ch2(train=False, return_mask=True, cache_ds=True, root=p.rrds_root, default_width=p.patch_width, default_height=p.patch_height)

elif p.dataset == "self2017":
    trainset = SingleImageDataset(glob.glob(p.self_pattern))
    #validationset = SingleImageDataset(glob.glob(p.self_pattern))
    validationset = Dibco.Dibco2017()
elif p.dataset == "dibco2010":
    trainset = Dibco.Dibco2009()
    validationset = Dibco.Dibco2010()
elif p.dataset == "dibco2011":
    trainset = Dibco.Dibco2009() + Dibco.Dibco2010()
    validationset = Dibco.Dibco2011()
elif p.dataset == "dibco2012":
    trainset = Dibco.Dibco2009() + Dibco.Dibco2010() + Dibco.Dibco2011()
    validationset = Dibco.Dibco2012()
elif p.dataset == "dibco2013":
    trainset = Dibco.Dibco2009() + Dibco.Dibco2010() + Dibco.Dibco2011() + Dibco.Dibco2012()
    validationset = Dibco.Dibco2013()
elif p.dataset == "dibco2014":
    trainset = Dibco.Dibco2009() + Dibco.Dibco2010() + Dibco.Dibco2011() + Dibco.Dibco2012() + Dibco.Dibco2013()
    validationset = Dibco.Dibco2014()
elif p.dataset == "dibco2016":
    trainset = Dibco.Dibco2009() + Dibco.Dibco2010() + Dibco.Dibco2011() + Dibco.Dibco2012() + Dibco.Dibco2013() + Dibco.Dibco2014()
    validationset = Dibco.Dibco2016()
elif p.dataset == "dibco2017":
    trainset = Dibco.Dibco2009() + Dibco.Dibco2010() + Dibco.Dibco2011() + Dibco.Dibco2012() + Dibco.Dibco2013() + Dibco.Dibco2014() + Dibco.Dibco2016()
    validationset = Dibco.Dibco2017()
elif p.dataset == "dibco2018":
    trainset = Dibco.Dibco2009() + Dibco.Dibco2010() + Dibco.Dibco2011() + Dibco.Dibco2012() + Dibco.Dibco2013() + Dibco.Dibco2014() + Dibco.Dibco2016() + Dibco.Dibco2017() + Dibco.Dibco2019()
    validationset = Dibco.Dibco2018()
elif p.dataset == "dibco2019":
    trainset = Dibco.Dibco2009() + Dibco.Dibco2010() + Dibco.Dibco2011() + Dibco.Dibco2012() + Dibco.Dibco2013() + Dibco.Dibco2014() + Dibco.Dibco2016() + Dibco.Dibco2017() + Dibco.Dibco2018()
    validationset = Dibco.Dibco2019()
elif p.dataset == "dibco2009":
    trainset = Dibco.Dibco2009()
    validationset = Dibco.Dibco2009()


trainset = tormentor.AugmentedDs(trainset, augmentation_factory=train_augmentation_factory,
                                          computation_device=p.tormentor_device, add_mask=True)
validationset = tormentor.AugmentedDs(validationset, augmentation_factory=val_augmentation_factory,
                                          computation_device=p.tormentor_device, add_mask=True)

trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=p.batch_size, num_workers=p.io_threads,drop_last=True)

valloader=torch.utils.data.DataLoader(validationset, shuffle=False, batch_size=1)

net = create_net(p.arch, p.n_channels, p.n_classes, .99, False, False)
param_hist, per_epoch_train_errors, per_epoch_validation_errors, start_epoch, net = resume(net, p.resume_fname, p.device)


optim = torch.optim.Adam(net.parameters(), lr=p.lr)
criterion = torch.nn.BCEWithLogitsLoss(reduction='none')


for epoch in range(start_epoch, p.epochs):
    print(f"Epoch {epoch}")
    if p.save_freq != 0 and epoch % p.save_freq==0:
        param_hist[epoch] = param_dict
        save(param_hist, per_epoch_train_errors, per_epoch_validation_errors,epoch,net)
    if p.validate_freq != 0 and epoch % p.validate_freq == 0:
        fscore,precision,recall, loss=run_epoch(p,p.val_device, valloader, net, criterion, optimizer=None, save_images=p.save_images, is_deeplabv3=False,save_input_images=p.save_input_images)
        per_epoch_validation_errors[epoch]=fscore,precision,recall,loss
    save_outputs=p.trainoutputs_freq != 0 and epoch % p.trainoutputs_freq == 0
    fscore, precision, recall, loss=run_epoch(p,p.device, trainloader, net, criterion, optimizer=optim, save_images=p.save_images, is_deeplabv3=False, save_input_images=p.save_input_images)
    per_epoch_train_errors[epoch]=fscore, precision, recall, loss
