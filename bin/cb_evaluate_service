#!/usr/bin/env python3
import requests
import fargv
from PIL import Image
import glob
import json
import cbdiadb
import cbphocnet
import numpy as np
from collections import defaultdict
from cbsegm import get_iou
import time
import sys
import pickle

p = {
    "port": 8080,
    "server": "127.0.0.1",
    "gt_json": set([]),
    "words": set([]),
    "db_root": "./data/fake_db_overlaid/",
    "output": "/tmp/results",
    "rectangles_per_document": 10000,
    "max_rectangles": 100000,
    "indexing_net": "./models/phocnet_0x0.pt",
    "iou_threshold": .2,
    "experiment_name":"NA",
    "output_mode": ("markdown", "latex", "stdio"),
    "print_queries": True,
    "save_data_path": f"/tmp/evaluate_{int(time.time()):010}.pickle",
    "save_data_freq": 10
}


def load_gt(json_file_list):#, embedding_net:cbphocnet.PHOCNet):
    per_page_rects = {}
    per_page_captions = {}
    all_captions = []
    for file in json_file_list:
        data = json.load(open(file, "r"))
        page_id = cbdiadb.filepath_to_pageid(file, name_depth=3)
        page_ltrb = []
        page_captions = []
        for n in range(len(data["rectangles_ltrb"])):
            if data["captions"][n].startswith("W@"):
                page_ltrb.append(data["rectangles_ltrb"][n])
                page_captions.append(data["captions"][n][2:])
                all_captions.append(data["captions"][n][2:].lower())
                #, "captions": np.array(page_captions)}
        per_page_captions[page_id] = np.array(page_captions)
        per_page_rects[page_id] = np.array(page_ltrb)
    vocabulary_freq = dict()
    for caption in all_captions:
        vocabulary_freq[caption] = vocabulary_freq.get(caption, 0) + 1
    return per_page_rects, per_page_captions, sorted(set(all_captions)), vocabulary_freq


def average_precision(answers, gt_per_page_rects, gt_per_page_captions, query, query_freq, iou_thr=.5):
    answers = answers["rectangles"]
    answers_per_page_rects = defaultdict(lambda:[])
    answers_per_page_idx = defaultdict(lambda: [])
    for idx, answer in enumerate(answers):
        answers_per_page_rects[tuple(answer[:2])].append(answer[2:])
        answers_per_page_idx[tuple(answer[:2])].append(idx)
    #print(sorted(gt_per_page_rects.keys()))
    #print(sorted(answers_per_page_idx.keys()))
    #assert set(answers_per_page_idx.keys()) >= set(gt_per_page_rects.keys())
    correct = np.zeros(len(answers), dtype=np.bool)
    iou_of_correct = np.zeros(len(answers), dtype=np.float)
    for page_id in gt_per_page_rects.keys():
        page_rects = np.array(answers_per_page_rects[page_id])
        gt_rectangles = gt_per_page_rects[page_id]
        if len(page_rects) and len(gt_rectangles):
            iou = get_iou(page_rects, gt_rectangles)

        # each GT object matched only once
        #iou[iou<(iou.max(axis=0)[None, :])] = 0

            nearest_words = np.argmax(iou, axis=1)
            caption_is_correct = gt_per_page_captions[page_id][nearest_words] == query
        else:
            caption_is_correct = gt_per_page_captions[page_id] != "INVALID"
        best_match_iou = np.max(iou, axis=1)
        iou_is_correct = best_match_iou > iou_thr
        iou_of_correct[np.array(answers_per_page_idx[page_id], dtype=np.long)] = best_match_iou
        correct[np.array(answers_per_page_idx[page_id], dtype=np.long)] = caption_is_correct * iou_is_correct
    #precision_at = np.cumsum(correct)/float(query_freq)

    possible_correct = np.cumsum(np.ones_like(correct))
    mask_possible_correct = possible_correct <= max(query_freq, correct.sum())
#    print(mask_possible_correct.shape,end="")
    possible_correct = np.cumsum(mask_possible_correct).astype(np.float)
#    print(mask_possible_correct.shape,end="")

    recall_at = np.cumsum(correct) / possible_correct
    precision_at = np.cumsum(correct) / np.cumsum(np.ones_like(correct))
    #print("\n",possible_correct[:5], precision_at[:5], np.nonzero(precision_at)[:5])

    if sum(correct):
        average_precision = precision_at[correct].mean()
    else:
        average_precision = 0.
    return(average_precision, precision_at, correct, iou_of_correct[correct])


def print_output(header, values, mode="markdown"):
    print("")
    if mode == "markdown":
        print("|"+"|".join(header)+"|")
        print("|" + "|".join(["-"*len(h) for h in header]) + "|")
        print("|" + "|".join([f"{v}" for v in values]) + "|")

    elif mode == "latex":
        print(" & ".join(header)+" \\\\")
        print("\\hr")
        print(" & ".join([f"{v}" for v in values]) + " \\\\")

    elif mode == "stdio":
        print(header)
        print([f"{v}" for v in values])

    else:
        raise ValueError



if __name__ == '__main__':
    p, _ = fargv.fargv(p)
    gt_per_page_rects, gt_per_page_captions, vocabulary, vocabulary_freq = load_gt(p.gt_json)
    if len(p.words) > 0:
        words = p.words
    else:
        words = vocabulary
    t = time.time()
    all_ap = np.zeros(len(vocabulary))
    precisions_at_1 = []
    precisions_at_5 = []
    precisions_at_10 = []
    precisions_at_100 = []
    recall_at_100 = []
    recall_at_10 = []
    total_time=0.

    accumulated_correct = []
    accumulated_queries = []
    accumulated_querie_occurence = []
    accumulated_iou = []


    for n, word in enumerate(words):
        url = f"http://{p.server}:{p.port}/searchword"
        query_dict = {"query": {"q_str": word},
                      "rectangles_per_document": p.rectangles_per_document,
                      "max_rectangles": p.max_rectangles}
        t = time.time()
        r = requests.post(url, json=query_dict)
        #print(f"{n:03}/{len(words)}: {word} {time.time()-t:0.5} sec. {word} Status:{r.status_code}")
        answers = r.json()
        total_time+=(time.time()-t)
        ap, precision_at, correct, iou_of_correct = average_precision(answers, gt_per_page_rects, gt_per_page_captions, word, vocabulary_freq[word], p.iou_threshold)

        all_ap[n] = ap
        precisions_at_1.append(precision_at[0])
        precisions_at_5.append(precision_at[4])
        precisions_at_10.append(precision_at[9])
        precisions_at_100.append(precision_at[99])
        nb_items = correct.sum()
        if nb_items < 1:
            nb_items = 1
        recall_at = np.cumsum(correct) / nb_items
        recall_at_10.append(recall_at[9])
        recall_at_100.append(recall_at[99])

        if p.print_queries:
            with np.printoptions(precision=3, suppress=True):
                print(f"{n:6} : {(total_time)/(n+1):.3f} : {repr(word):17} #{vocabulary_freq[word]}: {100*ap:.3f}\t cummulative mAP:{100*(all_ap[:n].mean()):.3f}\tTop-3 p@:",
                      precision_at[:3]*100, "\tTop-3 IoU:", iou_of_correct[:3], "\tRecall @100:", recall_at_100[-1],  "\tRecall @end:", recall_at[-1], "Indexes")
                sys.stdout.flush()
        accumulated_correct.append(np.nonzero(correct)[0].tolist())
        accumulated_queries.append(word)
        accumulated_querie_occurence.append(vocabulary_freq[word])
        accumulated_iou.append(iou_of_correct.tolist())
        if n % p.save_data_freq == 0 or n == len(vocabulary)-1:
            pickle.dump({"cli_params":p, "correct":accumulated_correct, "queries":accumulated_queries.append(word), "query_freq": accumulated_querie_occurence,"iou": accumulated_iou},open(p.save_data_path,"wb"))

    header = ["Name", "Query Count", "mAP", "Accuracy", "Precision @ 5", "Precision @10", "Precision @100", "Recall @10", "Recall @100", "Computation Time"]
    values = [f"{p.experiment_name}", f"{len(vocabulary_freq)}", f"{100*all_ap.mean():.3f}", f"{100*np.mean(precisions_at_1):.3f}",
              f"{100*np.mean(precisions_at_5):.3f}", f"{100*np.mean(precisions_at_10):.3f}",f"{100*np.mean(precisions_at_100):.3f}",
              f"{100*np.mean(recall_at_10):.3f}", f"{100*np.mean(recall_at_100):.3f}", f"{(total_time)/len(words):.3f}"]
    #print(f"mAP of {len(vocabulary_freq)} queries: {100*all_ap.mean()}")
    print_output(header, values, mode=p.output_mode)
