#!/usr/bin/env python3

import fargv
import tqdm
import pathlib
import os
import cbdiadb
import glob
import pathlib

def _absolute(path):
    return str(pathlib.Path(path).resolve())

def binarize_cmd(cb_root, in_img_list, binnet_path, out_postfix, max_device_mp, other_dependencies=""):
    all_in_imgs = ' '.join(in_img_list)
    dependencies = f"{binnet_path} {all_in_imgs} {other_dependencies}"
    cmd = f"{cb_root}/bin/cb_binarize -max_device_mp {float(max_device_mp)} -input {all_in_imgs} -resume_fname {binnet_path} -cache_binary_postfix {out_postfix}" + "\n\n"
    targets = [f"{in_img}{out_postfix}" for in_img in in_img_list]
    return targets, dependencies, cmd


def propose_words_cmd(cb_root, prob_img_list, proposal_params, out_postfix=".words.json", box_model="./models/box_iou.pt", device="cpu", other_dependencies="", parallel=1):
    def base_page_path(p):
        return os.path.splitext(os.path.splitext(os.path.splitext(p)[0])[0])[0]
    targets = [f"{base_page_path(prob_img)}{out_postfix}" for prob_img in prob_img_list]
    dependencies = f"{' '.join(prob_img_list)} {box_model} {other_dependencies}"
    if parallel > 1:
        cmd = f"ls {' '.join(prob_img_list)} | parallel -j {parallel}  {cb_root}/bin/cb_propose_words -prob_images {{}} -target_postfix {out_postfix} -box_model {box_model} -device {device} " + proposal_params + "\n\n"
    else:
        cmd = f"{cb_root}/bin/cb_propose_words -prob_images {prob_img_list} -target_postfix {out_postfix} -box_model {box_model} -device {device}" + proposal_params + "\n\n"
    return targets, dependencies, cmd


def embed_words_cmd(cb_root, db_root, docname, phocnet_path, image_files, words_files, output_basename, output_postfix=".pickle", device="cuda", other_dependencies=""):
    doc_short_name = docname.split("/")[-1]
    if doc_short_name == "":
        doc_short_name = docname.split("/")[-2]
    targets = [f"{output_basename}/{doc_short_name}{output_postfix}"]
    dependencies = ' '.join(image_files + words_files + [phocnet_path])
    dependencies = f"{dependencies} {other_dependencies}"
    cmd = f"{cb_root}/bin/cb_embed_proposals -phocnet {phocnet_path} -docname {docname} -db_root {db_root} -image_files {' '.join(image_files)} -words_files {' '.join(words_files)} -output_basename {output_basename} -device {device}" + "\n\n"
    return targets, dependencies, cmd


def render_makefile(chronicle_name, device, cb_root, db_root, binnet_path, boxnet_path, phocnet_path, binarization_postfix, max_device_mp, proposals_postfix, proposal_params, parallel_proposals, archive_location, archive_postfix, pythonpath, aux_proposals_postfix, add_clean):
    pythonpath = ":".join([_absolute(ppath) for ppath in pythonpath.split(":")])
    db_root = _absolute(db_root)
    cb_root = _absolute(cb_root)
    binnet_path = _absolute(binnet_path)
    boxnet_path = _absolute(boxnet_path)
    phocnet_path = _absolute(phocnet_path)
    input_images = glob.glob(f"{db_root}/{chronicle_name}/*.jp2")

    prob_images, bin_dependencies, bin_cmd = binarize_cmd(cb_root=cb_root, in_img_list=input_images, binnet_path=binnet_path, out_postfix=binarization_postfix, max_device_mp=max_device_mp)
    makefile_str = f"\n{' '.join(prob_images)}:{bin_dependencies}\n\t{bin_cmd}"

    proposals, proposal_dependencies, proposals_cmd = propose_words_cmd(cb_root=cb_root, prob_img_list= prob_images, out_postfix=proposals_postfix, box_model=boxnet_path, device=device, parallel=parallel_proposals, proposal_params=proposal_params)
    makefile_str += f"\n{' '.join(proposals)}:{proposal_dependencies}\n\t{proposals_cmd}\n"

    document_archives, document_dependencies, document_cmd = embed_words_cmd(cb_root=cb_root, db_root=db_root, docname=chronicle_name, phocnet_path=phocnet_path, image_files=input_images, words_files=proposals, output_basename=archive_location, output_postfix=archive_postfix, device=device)
    makefile_str += f"\n{document_archives[0]}:{document_dependencies}\n\t{document_cmd}\n"

    if aux_proposals_postfix != "":
        aux_proposals, proposal_dependencies, aux_proposals_cmd = propose_words_cmd(cb_root=cb_root,
                                                                                    prob_img_list=prob_images,
                                                                                    out_postfix=aux_proposals_postfix,
                                                                                    box_model=boxnet_path,
                                                                                    device=device,
                                                                                    parallel=parallel_proposals,
                                                                                    proposal_params=proposal_params)
        makefile_str += f"\n{' '.join(aux_proposals)}:{proposal_dependencies}\n\t{aux_proposals_cmd}\n"
        all_dependencies = ' '.join(aux_proposals + document_archives)
    else:
        all_dependencies = ' '.join(document_archives)

    makefile_str = f".EXPORT_ALL_VARIABLES:\n\nPYTHONPATH = {pythonpath}\n\n" + makefile_str

    makefile_str = f"ALL: {all_dependencies}\n\n" + makefile_str

    makefile_str += f"clean_bin:\n\trm {' '.join(prob_images)}\n\n"
    makefile_str += f"clean_boxes:\n\trm {' '.join(proposals)}\n\n"
    makefile_str += f"clean_embeddings:\n\trm {document_archives[0]}\n\n"
    if add_clean:
        if aux_proposals_postfix == "":
            makefile_str += f"clean: clean_bin clean_boxes clean_embeddings\n\techo 'Removing all'\n"
        else:
            makefile_str += f"clean_aux_boxes:\n\trm {' '.join(aux_proposals)}\n\n"
            makefile_str += f"clean: clean_bin clean_boxes clean_embeddings clean_aux_boxes\n\techo 'Removing all'\n"

    return makefile_str, document_archives[0]


p = {
    "chronicle_paths": set(glob.glob("./data/fake_db/*/*/*/*")),
    "make_filename": "Makefile",
    "device": "cuda",
    "cb_root": "./",
    "db_root": "./data/fake_db/",
    "binnet_path": "./models/srunet.pt",
    "boxnet_path": "./models/box_iou.pt",
    "phocnet_path": "./models/phocnet_0x0.pt",
    "binarization_postfix": ".bin.png",
    "max_device_mp": 20.,
    "proposals_postfix": ".words.pickle",
    "aux_proposals_postfix": ("", "Set this to '.words.json' to add json proposals as well"),
    "archive_postfix": ".pickle",
    "archive_location": "./data/idx_fake_db/",
    "parallel_proposals": 12,
    "add_clean": False,
    "proposal_params": " -rlsa_distances [0,2,4,6,12,18,24] -thresholds [32,64,128,224] -nms_threshold .6 -min_word_length 8 -min_word_height 6 -max_word_length .2 -max_word_height .1 ",
    "pythonpath": "./:./thirdparty/iunets/",
    "makefile_shards":1,
}


if __name__ == "__main__":
    p, _ = fargv.fargv(p, return_type="dict")
    total_makefile_str = ""
    pathlib.Path(p["archive_location"]).mkdir(parents=True, exist_ok=True)
    all_indexes = []
    for chronicle_path in tqdm.tqdm(p["chronicle_paths"]):
        chronicle_path = _absolute(chronicle_path)
        p["db_root"] = _absolute(p["db_root"])
        p["archive_location"] = _absolute(p["archive_location"])
        makefile_path = f"{chronicle_path}/{p['make_filename']}"
        kwargs = {}
        kwargs.update(p)
        makefile_shards = p["makefile_shards"]
        del kwargs["make_filename"]
        del kwargs["chronicle_paths"]
        del kwargs["makefile_shards"]
        kwargs["chronicle_name"] = cbdiadb.documentpath_to_documentid(documentpath=chronicle_path, document_root=p["db_root"])
        makefile_content, idx_file = render_makefile(**kwargs)
        open(makefile_path, "wt").write(makefile_content)
        total_makefile_str += f"\n{idx_file}: {p['phocnet_path']} {p['boxnet_path']} {p['binnet_path']}\n\tmake -f {makefile_path}\n\n"
        all_indexes.append(idx_file)
    print("Reducing redundants:", len(all_indexes),len(set(all_indexes)))
    all_indexes = sorted(set(all_indexes))
    if p["makefile_shards"] == 1:
        shards = {f"{p['archive_location']}/Makefile": all_indexes}
    else:
        shards = {}
        for shard_n in range(makefile_shards):
            shard_indexes = [all_indexes[n] for n in range(len(all_indexes)) if n % makefile_shards==shard_n]
            shards[f"{p['archive_location']}/makefile_{shard_n}_of_{makefile_shards}.mak"] = shard_indexes
    for makefile_name, indexes in shards.items():
        shard_makefile_str = "ALL: " + " ".join(all_indexes) + "\n\n" + total_makefile_str  # Todo (anguelos) remove redundant entries
        if p["add_clean"]:
            shard_makefile_str += f"\nclean:\n\trm {' '.join(all_indexes)}\n\n"
        open(makefile_name, "w").write(shard_makefile_str)
