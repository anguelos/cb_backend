#!/usr/bin/env python3

import cbphocnet
import cbdiadb
import fargv
import glob
import numpy as np
import tqdm
import json
import pickle
import time
import hashlib
import pathlib

from PIL import Image, ImageFile

ImageFile.LOAD_TRUNCATED_IMAGES = True
# This fixes the 'OSError: broken data stream when reading image file'
#https://stackoverflow.com/questions/42462431/oserror-broken-data-stream-when-reading-image-file
# examples of broken files:
#chronicle/soap-kv/soap-kv_00318_mesto-touzim-1947-1948/soap-kv_00318_mesto-touzim-1947-1948_0450.jp2
#chronicle/soap-kv/soap-kv_00246_obec-jakubov-1958/soap-kv_00246_obec-jakubov-1958_0600.jp2

p = {
     "phocnet":"./models/phocnet_0x0.pt",
     "docname":"blovice",
     "db_root":"./data/fake_db",
     "image_glob":"{db_root}/{docname}/*jp2",
     "words_glob":"{db_root}/{docname}/*.words.pickle",
     "image_files": set([]),
     "words_files": set([]),
     "output_basename":"./data/compiled_fake_db/{docname}",
     "output_postfix":".pickle",
     "append_boxcount_to_output": False,
     "batch_size": 1,
     "device": "cuda"
}


def compile_page(net, page_path, words_path, batch_size, device):
    img = Image.open(page_path)
    if words_path.endswith(".json"):
        rectangles_ltrb = np.array(json.load(open(words_path,"r"))["rectangles_ltrb"], dtype=np.int32)
    elif words_path.endswith(".pickle"):
        rectangles_ltrb = pickle.load(open(words_path, "rb"))
    else:
        raise NotImplementedError
    boxes, embeddings = net.embed_rectangles(img=img, ltrb=rectangles_ltrb, device=device, batch_size=batch_size)
    page_num = cbdiadb.imagepath_to_pagenum(page_path)
    page_width, page_height = img.size
    return page_num, boxes, embeddings, page_width, page_height


if __name__ == "__main__":
    p, _ = fargv.fargv(p)
    t = time.time()
    net_path = str(pathlib.Path(p.phocnet).resolve())
    net, _ = cbphocnet.resume_embedder(net_path)
    net_hash = hashlib.md5()
    with open(p.phocnet, "rb") as f:
        for chunk in iter(lambda: f.read(1024**2), b""):
            net_hash.update(chunk)
    net_hash = net_hash.hexdigest()
    if p.image_files == p.words_files == set([]):
        image_files = glob.glob(p.image_glob)
        words_files = glob.glob(p.words_glob)
    else:
        image_files = p.image_files
        words_files = p.words_files
    pageid2page = cbdiadb.pageids_to_filepaths(image_files, db_root=p.db_root)
    pageid2words = cbdiadb.pageids_to_filepaths(words_files, db_root=p.db_root)
    assert pageid2page.keys() == pageid2words.keys()
    all_page_numbers = [] # not page count, page integers consistent with the database
    all_boxes = []
    all_embeddings = []
    global_page_sizes = {}
    for page_id in tqdm.tqdm(pageid2page.keys(), desc=f"Embedings {p.docname}"):
        page_num, boxes, embeddings, page_width, page_height = compile_page(net, pageid2page[page_id], pageid2words[page_id], p.batch_size, p.device)
        all_page_numbers.append([page_num]*boxes.shape[0])
        all_boxes.append(boxes)
        all_embeddings.append(embeddings)
        global_page_sizes[page_id] = (page_width, page_height)
    all_page_numbers = np.concatenate(all_page_numbers, axis=0)
    all_boxes = np.concatenate(all_boxes, axis=0)
    all_embeddings = np.concatenate(all_embeddings, axis=0)
    document_id = cbdiadb.documentpath_to_documentid(image_files[0], p.db_root)
    outputs = {"page_nums": all_page_numbers, "boxes": all_boxes, "embeddings":all_embeddings,
               "netarch_hash": net.arch_hash(), "page_sizes":global_page_sizes,
               "generation_params":p, "netstate_hash":net_hash, "document_id": document_id, "embedding_net":net_path}
    if p.append_boxcount_to_output:
        fname_data_description = f".P{len(image_files):06}_B{all_boxes.shape[0]:08}_SZ{all_embeddings.shape[1]:06}"
    else:
        fname_data_description = f".SZ{all_embeddings.shape[1]:06}"
    output_filename = f"{p.output_basename}{p.output_postfix}"
    with open(output_filename, "wb") as fd:
        pickle.dump(outputs, fd)
    print(f"Computed {all_boxes.shape[0]} from {len(image_files)} pages in {(time.time() - t):7.3} sec.")
